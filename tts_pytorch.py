# -*- coding: utf-8 -*-
"""tts_pytorch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/kyutai-labs/delayed-streams-modeling/blob/main/tts_pytorch.ipynb
"""


import numpy as np
import torch
import os
from huggingface_hub import snapshot_download
from moshi.models.loaders import CheckpointInfo
from moshi.models.tts import DEFAULT_DSM_TTS_REPO, DEFAULT_DSM_TTS_VOICE_REPO, TTSModel

import soundfile as sf

# Configuration
# Force offline loading from local Hugging Face cache if available
os.environ.setdefault("HF_HUB_OFFLINE", "1")
os.environ.setdefault("HF_HOME", "/Users/jasneetsinghsawhney/.cache/huggingface")
os.environ.setdefault("HUGGINGFACE_HUB_CACHE", "/Users/jasneetsinghsawhney/.cache/huggingface/hub")
os.environ.setdefault("HF_HUB_DISABLE_TELEMETRY", "1")
os.environ.setdefault("HF_HUB_ENABLE_HF_TRANSFER", "0")
text = "Bonjour. DÃ©solÃ© si je pronoce votre nom incorrect."
voice = "expresso/ex03-ex01_happy_001_channel1_334s.wav"
print(f"See https://huggingface.co/{DEFAULT_DSM_TTS_VOICE_REPO} for available voices.")

# Set everything up
print("Loading TTS model...")
print("This may take a few minutes on first run as it downloads the model from Hugging Face.")
# Prefer the locally downloaded repo snapshot. This will resolve via HF cache when offline.
# If you downloaded kyutai/tts-1.6b-en_fr (or a compatible local snapshot), this will load it from cache.
PREFERRED_TTS_REPO = "kyutai/tts-1.6b-en_fr"

# Diagnose local snapshot availability
print("Resolving local HF snapshot for TTS repo...")
local_snapshot_path = None
try:
    local_snapshot_path = snapshot_download(PREFERRED_TTS_REPO, local_files_only=True)
    print(f"Found local snapshot: {local_snapshot_path}")
except Exception as e:
    print(f"Local snapshot not found for {PREFERRED_TTS_REPO}: {e}")
    print("Falling back to default repo; may require internet if not cached.")

# Use the repo id (offline mode + cache ensures local snapshot is used)
checkpoint_info = None
try:
    print(f"Using TTS repo: {PREFERRED_TTS_REPO}")
    checkpoint_info = CheckpointInfo.from_hf_repo(PREFERRED_TTS_REPO)
except Exception as e:
    print(f"Failed to load {PREFERRED_TTS_REPO} from cache: {e}")
    # Fallback to the default repo if the preferred one is unavailable in cache
    checkpoint_info = CheckpointInfo.from_hf_repo(DEFAULT_DSM_TTS_REPO)
print("Model checkpoint loaded. Initializing TTS model...")
tts_model = TTSModel.from_checkpoint_info(
    checkpoint_info, n_q=32, temp=0.6, device=torch.device("cpu")
)
print("TTS model initialized successfully!")

# If you want to make a dialog, you can pass more than one turn [text_speaker_1, text_speaker_2, text_2_speaker_1, ...]
print("Preparing script and voice...")
entries = tts_model.prepare_script([text], padding_between=1)
# If a local absolute path is provided for voice, use it directly to avoid online fetch
if os.path.isfile(voice):
    voice_path = voice
else:
    voice_path = tts_model.get_voice_path(voice)
print(f"Using voice: {voice}")
# CFG coef goes here because the model was trained with CFG distillation,
# so it's not _actually_ doing CFG at inference time.
# Also, if you are generating a dialog, you should have two voices in the list.
condition_attributes = tts_model.make_condition_attributes([voice_path], cfg_coef=2.0)
print("Voice and script prepared successfully!")

print("Generating audio...")

pcms = []


def _on_frame(frame):
    step_num = len(pcms) + 1
    print(f"Generating audio... Step {step_num}", end="\r")
    if (frame != -1).all():
        pcm = tts_model.mimi.decode(frame[:, 1:, :]).cpu().numpy()
        pcms.append(np.clip(pcm[0, 0], -1, 1))


# You could also generate multiple audios at once by extending the following lists.
print("Starting audio generation...")
all_entries = [entries]
all_condition_attributes = [condition_attributes]
with tts_model.mimi.streaming(len(all_entries)):
    result = tts_model.generate(
        all_entries, all_condition_attributes, on_frame=_on_frame
    )

print("\nAudio generation completed!")
print("Processing audio data...")
audio = np.concatenate(pcms, axis=-1)

# Save the audio to a file
output_filename = "generated_audio.wav"
print(f"Saving audio to: {output_filename}")
sf.write(output_filename, audio, tts_model.mimi.sample_rate)
print(f"âœ… Success! Audio saved to: {output_filename}")
print(f"ðŸ“Š Audio duration: {len(audio) / tts_model.mimi.sample_rate:.2f} seconds")
print(f"ðŸŽµ Sample rate: {tts_model.mimi.sample_rate} Hz")

